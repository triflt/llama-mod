# Mixture of Depths
An unofficial implementation of ["Mixture-of-Depths: Dynamically allocating compute in transformer-based language models"](https://arxiv.org/abs/2404.02258)
Inspired by [meta-llama](https://github.com/meta-llama/llama) and [sramshetty/mixture-of-depths](https://github.com/sramshetty/mixture-of-depths)

## Quick Start

1. Look how to setup env for Llama 2 [here](https://github.com/meta-llama/llama)

2. Install reqirements
```bash
pip install -r "reqirements.txt"
```

3. Try out the notebook `training.ipynb`

## Results:

- 
- 
- 

