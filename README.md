# Mixture of Depths
An unofficial implementation of ["Mixture-of-Depths: Dynamically allocating compute in transformer-based language models"](https://arxiv.org/abs/2404.02258)

## Quick Start

1. How to setup env for Llama 2 [here](https://github.com/meta-llama/llama)

2. pip install -r "reqirements.txt"

3. Try out the notebook train.py

## Results:

- 
- 
- 

