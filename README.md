# Mixture of Depths
Comparison ["Mixture-of-Depths"](https://arxiv.org/abs/2404.02258) with default [Llama](https://arxiv.org/abs/2302.13971) architecture.
Inspired by [meta-llama](https://github.com/meta-llama/llama) and [sramshetty/mixture-of-depths](https://github.com/sramshetty/mixture-of-depths)

## Start

1. Look how to setup env for Llama 2 [here](https://github.com/meta-llama/llama)

2. Install reqirements
```bash
pip install -r "reqirements.txt"
```

3. Try out the notebook `training.ipynb`

## Result:
[Comparison](images/comparison.png)
- The value of the loss function is up to 40% lower for the MoD architecture than for the default Llama.

